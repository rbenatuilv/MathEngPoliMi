\chapter{Numerical Linear Algebra tools}

\section{Introduction: Recap of Linear Algebra}

In this section we will review some basic concepts of Linear Algebra that will be useful for the rest of the course.

\subsection{Matrix-vector multiplication}

Given a matrix $A \in \mathbb{R}^{m \times n}$ and a vector $x \in \mathbb{R}^n$, the matrix-vector multiplication $y = Ax$ 
is defined as:

\begin{equation}
    y_i = \sum_{j=1}^{n} A_{ij} x_j
\end{equation}

A matrix-vector multiplication can be considered as a linear combination of the columns of the matrix $A$. Lets
see an example:

\begin{equation}
    \begin{bmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2 \\
    \end{bmatrix} = \begin{bmatrix}
        1 \\
        2 \\
        3
    \end{bmatrix} x_1 + \begin{bmatrix}
        4 \\
        5 \\
        6
    \end{bmatrix} x_2
\end{equation}

\subsection{Column space of a matrix}

The column space of a matrix $A \in \mathbb{R}^{m \times n}$ is the subspace of $\mathbb{R}^m$ spanned by the columns of $A$.
In other words, it is the set of all possible linear combinations of the columns of $A$. The column space of a matrix is denoted
as $C(A)$.\\

If the columns of $A$ are linearly independent, then the column space of $A$ is the entire $\mathbb{R}^m$. If the columns of $A$
are linearly dependent, then the column space of $A$ is a subspace of $\mathbb{R}^m$ with dimension equal to the rank of $A$.\\

The rank of a matrix $A$ is is the size of the largest set of linearly independent columns of $A$. It is denoted as $rank(A)$.
Note that $rank(A) = rank(A^T)$.

\subsection{System of linear equations}

A system of linear equations is a set of $m$ equations with $n$ unknowns of the form:

\begin{equation}
    \begin{aligned}
        a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &= b_1 \\
        a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &= b_2 \\
        \vdots \\
        a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &= b_m \\
    \end{aligned}
\end{equation}

This system can be written in matrix form as $Ax = b$, where $A \in \mathbb{R}^{m \times n}$, $x \in \mathbb{R}^n$ 
and $b \in \mathbb{R}^m$.\\

The system $Ax = b$ has a solution if and only if $b \in C(A)$. If $b \in C(A)$, then the system has a unique solution
if and only if $rank(A) = n$. If $rank(A) < n$, then the system has infinitely many solutions.

\subsection{CR factorization}

The CR factorization of a matrix $A \in \mathbb{R}^{m \times n}$, with $m \geq n$, is a factorization of $A$ as $A = CR$, where
$C \in \mathbb{R}^{m \times r}$ is a matrix with the linearly independent columns of $A$ and $R \in \mathbb{R}^{r \times n}$ is 
obtained by determining the coefficients of the linear combination of the columns of $C$ that give the columns of $A$.
In this factorization, $r = rank(A)$.\\

Lets see an example:

\begin{equation}
    A = \begin{bmatrix}
        1 & 4 & 7 \\
        2 & 5 & 8 \\
        3 & 6 & 9
    \end{bmatrix} = \begin{bmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{bmatrix} \begin{bmatrix}
        1 & 0 & -1 \\
        0 & 1 & 2
    \end{bmatrix} = C R
\end{equation}

The matrix $C$ is also called the Row Reduced Echelon Form of $A$.

\subsection{Matrix-matrix multiplication}

Given two matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$, the matrix-matrix multiplication $C = AB$
is defined as:

\begin{equation}
    C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}
\end{equation}

A matrix-matrix multiplication can be considered as the outer product of the columns of $A$ and the rows of $B$. Lets see an example:

\begin{equation}
    \begin{bmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{bmatrix} \begin{bmatrix}
        1 & 2 \\
        3 & 4
    \end{bmatrix} = \begin{bmatrix}
        1 \\
        2 \\
        3
    \end{bmatrix} \begin{bmatrix}
        1 & 2
    \end{bmatrix} + \begin{bmatrix}
        4 \\
        5 \\
        6
    \end{bmatrix} \begin{bmatrix}
        3 & 4
    \end{bmatrix}
\end{equation}

Note that each outer product generates a matrix of the same size as the result matrix, but always with rank 1. So 
the matrix-matrix multiplication can be considered as a sum of rank 1 matrices, obtained by the outer products of the columns of $A$
and the rows of $B$.

\subsection{Null space of a matrix}

The null space of a matrix $A \in \mathbb{R}^{m \times n}$ is the set of all vectors $x \in \mathbb{R}^n$ such that $Ax = 0$.
The null space of a matrix is denoted as $N(A)$. It is also called the kernel of $A$, denoted as $ker(A)$.\\

Formally, we have that:

\begin{equation}
    N(A) = \{ x \in \mathbb{R}^n : Ax = 0 \}
\end{equation}

The null space of a matrix is a subspace of $\mathbb{R}^n$. The dimension of the null space of a matrix is called the nullity 
of the matrix.

\subsection{Fundamental subspaces of a matrix}

Given a matrix $A \in \mathbb{R}^{m \times n}$, we can define four fundamental subspaces:

\begin{itemize}
    \item The column space of $A$, denoted as $C(A)$
    \item The row space of $A$, denoted as $C(A^T)$
    \item The null space of $A$, denoted as $N(A)$
    \item The left null space of $A$, denoted as $N(A^T)$
\end{itemize}

These subspaces are related by the following properties:

\begin{equation}
    \begin{aligned}
        C(A) &\perp N(A^T) \\
        C(A^T) &\perp N(A) \\
    \end{aligned}
\end{equation}

They also satisfy the following dimensions properties:

\begin{equation}
    \begin{aligned}
        dim(C(A)) + dim(N(A)) &= n \\
        dim(C(A^T)) + dim(N(A^T)) &= m \\
    \end{aligned}
\end{equation}

This is known as the Rank-Nullity Theorem.

\subsection{Orthogonal matrices}

An orthogonal matrix is a square matrix $Q \in \mathbb{R}^{n \times n}$ such that $Q^T Q = I$, where $I$ is the identity matrix.
This implies that $Q^T = Q^{-1}$.\\

Now, consider that $Q$ is an orthogonal matrix, and set $w = Q^T x$. Then we have that:

\begin{equation}
    \begin{aligned}
        \|w\|^2 =w^T w &= x^T Q Q^T x \\
        &= x^T x = \|x\|^2
    \end{aligned}
\end{equation}

This means that the norm of a vector is preserved under an orthogonal transformation. This is called an isometry. It is
a useful property for numerical algorithms, as it helps to avoid numerical instability.\\

There are two main types of orthogonal transformations that we are interested:

\subsubsection{Rotation matrices}

A rotation matrix is an orthogonal matrix that represents a rotation in $\mathbb{R}^2$ or $\mathbb{R}^3$. In $\mathbb{R}^2$,
a rotation matrix is of the form:

\begin{equation}
    Q(\theta) = \begin{bmatrix}
        \cos(\theta) & -\sin(\theta) \\
        \sin(\theta) & \cos(\theta)
    \end{bmatrix}
\end{equation}

\subsubsection{Reflection matrices}

A reflection matrix is an orthogonal matrix that represents a reflection with respect to a hyperplane. If $n$ denotes
the unit normal vector to the hyperplane, then the reflection matrix is of the form:

\begin{equation}
    Q = I - 2 n n^T
\end{equation}

Note that the inverse of this matrix is itself, as $Q^T = Q^{-1}$ and in this case, $Q$ is symmetric ($Q = Q^T$).\\

\subsection{QR factorization}

The QR factorization of a matrix $A \in \mathbb{R}^{m \times n}$, with $m \geq n$, is a factorization of $A$ as $A = QR$, where
$Q \in \mathbb{R}^{m \times n}$ is an orthogonal matrix and $R \in \mathbb{R}^{n \times n}$ is an upper triangular matrix.\\

\subsubsection{Gram-Schmidt process}

The Gram-Schmidt process is a method to compute the QR factorization of a matrix. Given a matrix $A \in \mathbb{R}^{m \times n}$,
the Gram-Schmidt process computes an orthonormal basis for the column space of $A$, as follows:

\begin{equation}
    \begin{aligned}
        q_1 &= \frac{a_1}{\|a_1\|} \\
        q_i = a_i - \sum_{j=1}^{i-1} &(q_j^T a_i) q_j \quad \forall i = 2, \ldots, n
    \end{aligned}
\end{equation}

where $a_i$ denotes the $i$-th column of $A$. The matrix $Q$ is obtained by stacking the vectors $q_i$ as columns. The matrix $R$
is obtained by computing the coefficients of the linear combination of the columns of $Q$ that give the columns of $A$.\\

\subsection{Eigenvalues and eigenvectors}

Given a square matrix $A \in \mathbb{R}^{n \times n}$, a scalar $\lambda$ is called an eigenvalue of $A$ if there exists a vector
$v \in \mathbb{R}^n$ such that:

\begin{equation}
    Av = \lambda v
\end{equation}

The vector $v$ is called an eigenvector of $A$ associated with the eigenvalue $\lambda$.\\

Let $P$ be the matrix whose columns are the eigenvectors of $A$, and $\Lambda$ be the diagonal matrix whose diagonal elements
are the eigenvalues of $A$. Then we have that:

\begin{equation}
    A = P \Lambda P^{-1}
\end{equation}

This is called the eigendecomposition of $A$.\\

The eigenvalues of a matrix are the roots of the characteristic polynomial of $A$, which is defined as:

\begin{equation}
    det(A - \lambda I) = 0
\end{equation}

\subsection{Similar matrices}

Two square matrices $A$ and $B$ are called similar if there exists a non-singular matrix $M$ such that:

\begin{equation}
    B = M^{-1} A M
\end{equation}

Similar matrices have the same eigenvalues, but not necessarily the same eigenvectors. Let $(\lambda, y)$ be 
an eigenpair of $B$, then we have:

\begin{equation}
    B y = \lambda y \Rightarrow M^{-1} A M y = \lambda y \Rightarrow A (M y) = \lambda (M y)
\end{equation}

This means that $M y$ is an eigenvector of $A$ associated with the eigenvalue $\lambda$. So, to obtain
the eigenvectors of $A$ from the eigenvectors of $B$, we need to multiply the eigenvectors of $B$ by $M$.\\

This property can be useful. For example, if we want to compute the eigenvalues of a matrix $A$, we can find
some transformation $M$ such that $M^{-1} A M = B$ is a simpler matrix to work with, usually a lower triangular
matrix. Then we can compute the eigenvalues of $B$ and obtain the eigenvalues of $A$. $M$ is obtained by the permutation
matrices to get from $A$ to $B$. The Givens and Householder transformations are examples of such method.

\section{Power method}

The power method is an iterative algorithm to compute the dominant eigenvalue of a matrix (i.e., the eigenvalue with the largest
magnitude). The algorithm is as follows:

\begin{algorithm}[H]
    \caption{Power method}
    \begin{algorithmic}[1]
        \State Choose a random vector $x^{(0)}$, s.t. $\|x^{(0)}\| = 1$
        \For{$k = 1, 2, \ldots$}
            \State $y^{(k)} = A x^{(k-1)}$
            \State $x^{(k)} = \frac{y^{(k)}}{\|y^{(k)}\|}$
            \State $\lambda^{(k)} = x^{(k)T} A x^{(k)}$
        \EndFor
    \end{algorithmic}
\end{algorithm}

The convergence rate is determined by the ratio of the largest eigenvalue to the 
second largest eigenvalue.

\subsection{Rayleigh quotient}

The Rayleigh quotient is the base of the power method algorithm. Given a matrix $A \in \mathbb{R}^{n \times n}$ and
a vector $x \in \mathbb{R}^n$, the Rayleigh quotient is defined as:

\begin{equation}
    R(x) = \frac{x^T A x}{x^T x}
\end{equation}

For every eigenpair $(\lambda, v)$ of $A$, we have that:

\begin{equation}
    R(v) = \frac{v^T A v}{v^T v} = \frac{v^T \lambda v}{v^T v} = \lambda
\end{equation}

This means that the Rayleigh quotient is equal to the eigenvalue associated with the eigenvector $v$. This property
is used in the power method to compute the dominant eigenvalue of a matrix.

\subsection{Proof of convergence for the power method}

The power method converges to the dominant eigenvalue of a matrix. The sketch proof is as follows:\\

Let $x^{(0)}$ be our initial vector, and let $\{v_1, \ldots, v_n\}$ be the eigenvectors of $A$. We can write $x^{(0)}$ as a linear
combination of the eigenvectors of $A$ (since the eigenvectors of $A$ form a basis of $\mathbb{R}^n$):

\begin{equation}
    x^{(0)} = \sum_{i=1}^{n} \alpha_i v_i
\end{equation}

Then we have that:

\begin{equation}
    A x^{(0)} = \sum_{i=1}^{n} \alpha_i A v_i = \sum_{i=1}^{n} \alpha_i \lambda_i v_i
\end{equation}

Since in every iteration $k$ of the power method we apply the matrix $A$ to the vector $x^{(k-1)}$, we have that:

\begin{equation}
    A x^{(k-1)} = A^k x^{(0)} = \sum_{i=1}^{n} \alpha_i \lambda_i^k v_i
\end{equation}

Now, let us factorize the previous equation by the dominant eigenvalue $(\lambda_1)^k$:

\begin{equation}
    A^k x^{(0)} = (\lambda_1)^k \left( \alpha_1 v_1 + \sum_{i=2}^{n} \alpha_i \left( \frac{\lambda_i}{\lambda_1} \right)^k v_i\right)
\end{equation}

Note that the term inside the parenthesis converges to zero as $k \rightarrow \infty$, since the ratio of the other eigenvalues
to the dominant eigenvalue is less than 1. This means that $x^(k)$ converges to the direction of the dominant eigenvector $v_1$.
When it is normalized, its Rayleigh quotient converges to the dominant eigenvalue $\lambda_1$.

\subsection{Inverse power method}

The inverse power method is an iterative algorithm to compute the eigenvalue with the smalles magnitude of a matrix. Note that the smallest
eigenvalue of a matrix is the largest eigenvalue of its inverse, since:

\begin{equation}
    A x = \lambda x \Rightarrow A^{-1} x = \frac{1}{\lambda} x
\end{equation}

The algorithm is similar to the power method, but instead of applying the matrix $A$ to the vector $x$, we apply the inverse
of the matrix $A$:

\begin{algorithm}[H]
    \caption{Inverse power method}
    \begin{algorithmic}[1]
        \State Choose a random vector $x^{(0)}$, s.t. $\|x^{(0)}\| = 1$
        \For{$k = 1, 2, \ldots$}
            \State $y^{(k)} = A^{-1} x^{(k-1)}$
            \State $x^{(k)} = \frac{y^{(k)}}{\|y^{(k)}\|}$
            \State $\lambda^{(k)} = x^{(k)T} A x^{(k)}$
        \EndFor
    \end{algorithmic}
\end{algorithm}

In practice, we normally don't compute the inverse of the matrix $A$, but instead solve the linear system $A x = y^{(k)}$ in each
iteration.

\subsection{Shifted inverse power method}

The shifted inverse power method is an iterative algorithm to compute the eigenvalue with the smalles magnitude of a matrix, but
with a shift $\mu$ added to the matrix $A$. The algorithm is as follows:

\begin{algorithm}[H]
    \caption{Shifted inverse power method}
    \begin{algorithmic}[1]
        \State Choose a random vector $x^{(0)}$, s.t. $\|x^{(0)}\| = 1$
        \For{$k = 1, 2, \ldots$}
            \State $y^{(k)} = (A - \mu I)^{-1} x^{(k-1)}$
            \State $x^{(k)} = \frac{y^{(k)}}{\|y^{(k)}\|}$
            \State $\lambda^{(k)} = x^{(k)T} A x^{(k)}$
        \EndFor
    \end{algorithmic}
\end{algorithm}

Note that with this algorithm, we are computing the eigenvalue of $A$ that is closest to the shift $\mu$. This can be useful
to compute the eigenvalues of a matrix that are close to a given value.

\section{Symmetric matrices}

A matrix $A \in \mathbb{R}^{n \times n}$ is called symmetric if $A = A^T$. Symmetric matrices have important properties:

\begin{itemize}
    \item The eigenvectors of a symmetric matrix form an orthonormal basis of $\mathbb{R}^n$.
    \item All the eigenvalues of a symmetric matrix are real.
\end{itemize}

Let us prove the first property:\\

Let $A$ be a symmetric matrix, and let $\lambda_1, \ldots, \lambda_n$ be its eigenvalues. Let $v_1, \ldots, v_n$ be the eigenvectors
associated with the eigenvalues $\lambda_1, \ldots, \lambda_n$. Let us take two eigenvectors $v_i$ and $v_j$, such that $i \neq j$.
Then we have that:

\begin{equation}
    A v_i = \lambda_i v_i \quad \text{and} \quad A v_j = \lambda_j v_j
\end{equation}

Then we have that:

\begin{equation*}
    (A - \lambda_i I) v_i = 0 \quad \text{and} \quad (A - \lambda_i I) v_j = (\lambda_j - \lambda_i) v_j 
\end{equation*}

\begin{equation}
    \Rightarrow v_i \in N(A - \lambda_i I) \quad \text{and} \quad v_j \in C(A - \lambda_i I)
\end{equation}

Since $A$ is symmetric, we have that $A - \lambda_i I$ is also symmetric. Then we have that:

\begin{equation}
    N(A - \lambda_i I) = N((A - \lambda_i)^T) \perp C(A - \lambda_i I)
\end{equation}

Concluding that $v_i$ and $v_j$ are orthogonal. Since this holds for all pairs of eigenvectors, we have that the eigenvectors
of a symmetric matrix form an orthonormal basis of $\mathbb{R}^n$.\\

Now, let us prove the second property:\\

Since $A$ is symmetric, we have that $A = A^T$. Then we have that:

\begin{equation}
    \begin{aligned}
        A x &= \lambda x \\
        A \bar{x} &= \bar{\lambda} \bar{x}
    \end{aligned}
\end{equation}

Then we have that:

\begin{equation}
    \begin{aligned}
        \bar{x}^T A x &= \lambda x^T x = \lambda \|x\|^2 \\
        x^T A \bar{x} &= \bar{\lambda} x^T \bar{x} = \bar{\lambda} \|x\|^2
    \end{aligned}
\end{equation}

Since $A = A^T$, we have that:

\begin{equation}
    \bar{x}^T A x = (A x)^T \bar{x} = x^T A^T \bar{x} = x^T A \bar{x}
\end{equation}

Then we have that:

\begin{equation}
    \lambda \|x\|^2 = \bar{\lambda} \|x\|^2 \Rightarrow \lambda = \bar{\lambda}
\end{equation}

This means that the eigenvalues of a symmetric matrix are real.

\subsection{Symmetric positive definite matrices}

A matrix $A \in \mathbb{R}^{n \times n}$ is called symmetric positive definite if it is symmetric and if for every vector $x \in \mathbb{R}^n$
we have that:

\begin{equation}
    x^T A x > 0 \quad \forall x \neq 0
\end{equation}

Symmetric positive definite matrices have important properties:

\begin{itemize}
    \item All the eigenvalues of a symmetric positive definite matrix are positive.
    \item The Cholesky factorization of a symmetric positive definite matrix exists and is unique:
    \begin{equation}
        A = L^T L
    \end{equation}
\end{itemize}

In fact, a symmetric matrix is positive definite if and only if all its eigenvalues are positive, so:

\begin{equation}
    x^T A x > 0 \quad \forall x \neq 0 \quad \Leftrightarrow \quad \lambda_i > 0 \quad \forall i
\end{equation}

Note that the quantity $x^T A x$ is called the energy of the vector $x$ with respect to the matrix $A$. This quantity is always positive
for a symmetric positive definite matrix.\\


\section{Singular Value Decomposition (SVD)}

The Singular Value Decomposition (SVD) is a factorization of a matrix $A \in \mathbb{R}^{m \times n}$ as:

\begin{equation}
    A = U \Sigma V^T
\end{equation}

where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m \times n}$
is a quasi-diagonal matrix with the singular values of $A$. The singular values of $A$ are the square roots of the eigenvalues of $A^T A$.\\

Note that if $rank(A) = r$, then the matrix $\Sigma$ has $r$ non-zero singular values, and the remaining singular values are zero. Assume that
the singular values of $A$ are $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0$. Then we have that:

\begin{equation}
    \begin{aligned}
        A v_i &= \sigma_i u_i \quad \forall i = 1, \ldots, r \\
        A v_i &= 0 \quad \forall i = r+1, \ldots, n \\
    \end{aligned}
\end{equation}

where $u_i$ and $v_i$ are the columns of $U$ and $V$, respectively. The vectors $u_i$ and $v_i$ are called the left and right singular vectors
of $A$, respectively.\\

The SVD can also be written as:

\begin{equation}
    A = \sum_{i=1}^{r} \sigma_i u_i v_i^T
\end{equation}

\subsection{Economy SVD}

The economy SVD is a factorization of a matrix $A \in \mathbb{R}^{m \times n}$ as:

\begin{equation}
    A = U_r \Sigma_r V_r^T
\end{equation}

where $U \in \mathbb{R}^{m \times r}$, $V \in \mathbb{R}^{n \times r}$ and $\Sigma \in \mathbb{R}^{r \times r}$, with $r = rank(A)$.\\

The economy SVD is useful when we are only interested in the first $r$ singular values of $A$, which are the non-zero singular values.\\

\subsection{Low-rank approximation}

The SVD can be used to compute a low-rank approximation of a matrix $A \in \mathbb{R}^{m \times n}$. Given a rank $k$, with $k < r = rank(A)$,
the low-rank approximation of $A$ is given by:

\begin{equation}
    A_k = U_k \Sigma_k V_k^T = \sum_{i=1}^{k} \sigma_i u_i v_i^T
\end{equation}

where $U_k \in \mathbb{R}^{m \times k}$, $V_k \in \mathbb{R}^{n \times k}$ and $\Sigma_k \in \mathbb{R}^{k \times k}$, with $k < r = rank(A)$.\\

Because the singular values of $A$ are sorted in decreasing order, the low-rank approximation only considers the first $k$ singular values of $A$,
as they are the components of $A$ with the largest contribution.\\

The low-rank approximation of a matrix is useful for data compression, as it allows to represent a matrix with a smaller number of parameters. 
Note that the low-rank approximation of a matrix is the best rank-$k$ approximation of the matrix in the Frobenius norm. This is called 
the Eckart-Young theorem.









