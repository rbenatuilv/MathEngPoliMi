{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Engineering\n",
    "## Exam preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Which are the four approaches to tame velocity? List them all before comparing and constrasting two of them\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "We have 4 main approaches to tame the velocity of the input data and stream processing:\n",
    "* Event-based systems (EBS): These systems focus on individual events as the unit of processing. Events are processed and responded to in near real-time, often by triggering specific actions based on predefined rules.\n",
    "\n",
    "* Data Stream Management Systems (DSMS): Similar to traditional database management systems, DSMS focuses on continuous queries over unbounded streams of data. It provides capabilities for querying, filtering and aggregating data streams in real-time.\n",
    "\n",
    "* Complex Event Processing (CEP): CEP goes beyond individual events by identifying patterns and correlations across multiple events. It allows for the detection of more sophisticated scenarios, such as trends or anomalies.\n",
    "  \n",
    "* Event-driven architectures (EDA): EDA s a design paradigm where the flow of data and control is determined by events. Components of the system sommunicate asynchronously through event messages, enabling scalability and flexibility in handling high-velocity data.\n",
    "\n",
    "For example, let us compare EBS and CEP:\n",
    "* Similarities:\n",
    "  * Event-centric processing: Both approaches revolve around processing events as the fundamental unit of data. Events are the primary triggers for actions in both systems.\n",
    "\n",
    "  * Real-time response: Both EPS ans CEP aim to process and respond to data in real-time or near real-time, making the suitable for dynamic and time-sensitive applications.\n",
    "\n",
    "  * Scalability: Both must handle high-velocity data streams efficiently, often requiring distributed architectures to scale horizontally.\n",
    "\n",
    "  * Application in EDAs: Both can be integrated into Event-Driven Architectures, where components communicate asynchronously via events.\n",
    "\n",
    "* Differences:\n",
    "  * Focus of processing: EBS deals with individual events in isolation, focusing on simple and direct reactions to each event. On the other hand, CEP processes multiple events collectively, identifying patterns, sequences or correlations.\n",
    "\n",
    "  * Complexity: EBS is relatively simple to design and implement, as it doesn't require advanced pattern recognition, while CEP is more complex, as it involves defining rules for detecting sequences, windows and relationships across events.\n",
    "\n",
    "  * Use cases: EBS is more suitable for reactive tasks like sending notifications, logging or device monitoring. CEP is better for scenarios requiring advanced analytics, such as fraud detection, supply chain optimization or trend analysis.\n",
    "\n",
    "  * State management: EBS is typically stateless, reacting to each event independently, while CEP is often stateful, requiring the system to maintain a memory of past events for pattern detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Which are the three time-models we introduced? List them all before comparing and constrasting two of them\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "We introduced the 3 following time models:\n",
    "* Stream-only time model: in this model, time is defined by the order of events in the stream. We only care about the order of events and not the time in which they happend. This model has no notion of time, limiting the amount of possible queries, but also reducing the latency.\n",
    "\n",
    "* Absolute time model: in this model, time is defined by the real time in which events happened. In this model, knowing the exact moment of the ocurrance of events is as important as their order. This increases the expresiveness of the model, but also the latency.\n",
    "\n",
    "* Interval-based time model: in this model, time is defined in intervals. It is useful when we need to know the time in which events happened, but also group them. This increases the expresiveness, but also the latency of the model.\n",
    "\n",
    "Let us compare stream-only and absolute time models:\n",
    "\n",
    "* Similarities:\n",
    "  * Both models focus on processing events as they arrive.\n",
    "\n",
    "  * Both are simpler than the interval-based, as they don't require reasoning about overlapping or continuous time ranges.\n",
    "\n",
    "* Differences:\n",
    "  * Time representation: stream-only model does not use explicit timestamps and relies only on the system clock to order the arrival of events. The absolute time model includes explicit timestamps for each event, enabling analysis based on actual occurrence times.\n",
    "\n",
    "  * Use cases: the stream-only model is suitable for scenarios where ordering is sufficient, such as reactive systems or message queues. On the other hand, absolute time model is ideal for scenarios requiring precise temporal analytics, such as financial transaction monitoring or sensor data analysis.\n",
    "  \n",
    "  * Expresiveness and latency: stream-only model allows a lower latency, but with a low expresiveness. In absolute time, we are able to perform more queries, but with the drawback of a higher latency.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What are the basic types of windows? List them all and describe the behavior of at least two of them\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The basic types of windows are:\n",
    "* Tumbling window: this window has a fixed size and does not overlap. In other words, it makes a partition of time, so that each event always belongs to only one window.\n",
    "\n",
    "* Hopping window: this window moves forward in fixed intervals, usually overlapping with the previous window. This produces that an event may belong to two (or more, depending on the value of the stride) hopping windows.\n",
    "\n",
    "* Session window: this window is determined dynamically based on periods of activity separated by periods of inactivity. In other words, an event belongs to a certain window if either is the first one detected after a period of inactivity, or falls within a limited time context after the ocurrance of another event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What is a context and how does it relate to interval-based time model? Give an example that supports your explanation.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A context refers to the temporal or logical scope within which events are considered relevant or processed. It defines the boundaries for evaluating events, aggregating data or detecting patterns.\n",
    "\n",
    "The interval-based model associates events with specific time intervals, during which they are valid (i.e., are taken into account for aggregation, processing, etc.). Contexts in the model are defined by these intervals, enabling the system to process events only within their relevant timeframes. \n",
    "\n",
    "For example, imagine we are monitoring the transactions of a user during the following 10 minutes after a transaction, for fraud detection. Then, the context is generated after the first transaction received (after a period of inactivity), and then it is mantained over an interval of 10 minutes (in which the new transactions that arrive are processed together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Why do we say that Complex Event Processing can tame the torrent effect? Support you claim with an example\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The torrent effect occurs when a high volume of events floods a system, making it difficult to process or extract meaningful information in real-time. Complex Event Processing (CEP) can tame this effect by aggregating, filtering and correlating low-level events into high-level meaningful insights. This is done by:\n",
    "\n",
    "* Pattern detection: CEP can detect specific sequences of events.\n",
    "* Temporal constraints: CEP can generate temporal contexts in which only the events within are valid.\n",
    "* Event filtering and aggregation, focusing only on specific data according to our needs.\n",
    "* Output control: CEP can regulate the frequency and form in which the results are generated, to avoid overwhelming outputs.\n",
    "\n",
    "For example, in fire alarm systems, sensors are continuously generating data like temperature, smoke and CO levels. This creates a flood of raw events that can overwhelm any system if not properly handled. CEP manages this by :\n",
    "* Filtering irrelevant data (e.g., temperatures lower than 50°).\n",
    "\n",
    "* Aggregating values over short time intervals (e.g., average temperature every 10 seconds).\n",
    "\n",
    "* Detecting specific patterns on a given context (e.g., after avg temperature higher than 50°, high smoke and CO within a minute).\n",
    "\n",
    "* Controlling the output (e.g., trigger once the alarm after the fire event is detected, not every single time after that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) What are latency / throughput? Given an example of both in the same domain\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We refer to latency as the time taken for a message to travel from the producer to the consumer. Throughput, on the other hand, is the number of messages that can be processed in a given time period.\n",
    "\n",
    "We can illustrate it with a visual example. Imagine we have a road connecting city A to B. We refer to latency as the time it takes a car to move from A to B, and throughput the amount of cars that arrive to B from A on, for example, an hour.\n",
    "\n",
    "In the same way, latency on an distributed streaming system refers to the time it takes to process a message, while throughput means the amount of messages (or info) that can be processed within a fixed amount of time, e.g., a minute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Describe the Kafka components at a conceptual, system, and physical level. Illustrate them using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "At a conceptual level:\n",
    "\n",
    "Kafka is a distributed system for real-time data streaming, composed of:\n",
    "\n",
    "* Producer: a system that sends data to Kafka topics. \n",
    "\n",
    "* Consumer: a system that reads data from certain Kafka topics.\n",
    "\n",
    "* Topic: is a category of messages in Kafka, a way to organize incoming messages based on their content.\n",
    "\n",
    "* Message: it is the unit of data in Kafka. It consists of a key-value pair that is sent from the produced and is stored on a topic\n",
    "\n",
    "* Cluster: a Kafka cluster manages a set of topics.\n",
    "\n",
    "At a system level, we add the following:\n",
    "\n",
    "* Partitions: each topic is subdivided into partitions, to allow for parallel processing and scalability. Each partition of the same topic is stored on different brokers.\n",
    "\n",
    "* Broker: it is a Kafka server that stores and serves data. It also handles replication of partitions to allow for fault-tolerance. Each broker can store multiple partitions\n",
    "\n",
    "At a physical level:\n",
    "\n",
    "* Each broker is hosted on different machines of a network.\n",
    "\n",
    "* When a message is received, it is assigned to a certain partition based on the key (round-robin if no key)\n",
    "\n",
    "* The message is replicated to other brokers, for fault tolerance. Usually, a partition is replicated many times. \n",
    "\n",
    "* Each partition is stored on the disk of the broker, in a log-structured format, to allow for fast reads and writes (commit log). Also we can configure retention policies to keep messages for an amount of time.\n",
    "\n",
    "* Consumers generally form consumer groups, that subscribe to the same topic. This allows parallel reading from many partitions.\n",
    "\n",
    "Example: Log Processing System\n",
    "\n",
    "Conceptual Level:\n",
    "\n",
    "* A logging service (Producer) sends application logs to a Kafka topic named logs.\n",
    "* Monitoring tools (Consumers) read logs from the topic to detect anomalies.\n",
    "\n",
    "System Level:\n",
    "\n",
    "* The logs topic is divided into 3 partitions for scalability.\n",
    "* Producers write logs into partitions in a round-robin fashion.\n",
    "* Consumers subscribe to the logs topic and are assigned specific partitions to process logs in parallel.\n",
    "* The Kafka cluster consists of 5 brokers, each storing one or more partitions of the logs topic.\n",
    "\n",
    "Physical Level:\n",
    "\n",
    "* The cluster is deployed on 5 physical or virtual machines (brokers).\n",
    "* Each broker has disk storage where partitions are stored (e.g., Partition 0 on Broker 1, Partition 1 on Broker 2, etc.).\n",
    "* Messages are replicated across brokers (e.g., Partition 0 has replicas on Broker 1 and Broker 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Illustrate the programming model of spark structured streaming at the logical and physical level. Illustrate them using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Logical Level:\n",
    "\n",
    "At the logical level, Spark Structured Streaming treats incoming live data streams as unbounded tables that are continuously updated as new data arrives. Computations are expressed as a Directed Acyclic Graph (DAG), resembling batch queries applied to static tables. Key characteristics:\n",
    "\n",
    "* Incremental Updates: The DAG processes only new data in each micro-batch.\n",
    "\n",
    "* State Management: Maintains and updates state across batches to ensure continuity.\n",
    "\n",
    "* Declarative API: Uses SQL-like syntax for operations, making streaming computation similar to batch processing.\n",
    "\n",
    "Physical level:\n",
    "\n",
    "At the physical level, Spark executes the streaming DAG incrementally on the latest available data:\n",
    "\n",
    "* Micro-Batches: The incoming data is processed as small, fixed-size batches.\n",
    "\n",
    "* Execution Engine: Utilizes the Catalyst optimizer and Tungsten execution engine for efficient query optimization and execution.\n",
    "\n",
    "\n",
    "* Fault Tolerance: Ensures exactly-once guarantees using checkpoints and write-ahead logs.\n",
    "\n",
    "Each micro-batch reads new data from the source, applies transformations, updates intermediate states (if needed), and writes results to the sink (e.g., database, Kafka, file system).\n",
    "\n",
    "\n",
    "### Illustrated Example: Real-Time Traffic Monitoring\n",
    "\n",
    "Logical Model:\n",
    "\n",
    "* Use Case: Monitoring vehicle speeds on a highway.\n",
    "* Query: Compute the average speed of vehicles passing through different sections of the highway every minute.\n",
    "\n",
    "Logical representation:\n",
    "\n",
    "$$\\texttt{speed\\_sdf.groupBy(window(\"timestamp\", \"1 minute\"), \"section\").agg(avg(\"speed\"))}$$\n",
    "\n",
    "The window function groups data into one-minute intervals.\n",
    "\n",
    "Physical Model:\n",
    "\n",
    "* Data is read from traffic sensors as micro-batches.\n",
    "* The engine updates state (e.g., total speed and count for each section).\n",
    "* Results are written incrementally to a dashboard showing average speeds in real time.\n",
    "\n",
    "This model allows seamless integration of batch and stream processing while ensuring fault tolerance and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) How does Spark Structured Stream treat late arrivals in windowed aggregation? Illustrate it using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Late arrivals in Spark Structured Streaming are events that arrive after the expected time window for processing. To manage these effectively, Spark uses watermarking, which:\n",
    "\n",
    "* Tracks the maximum event time seen in the stream.\n",
    "* Specifies a watermark delay as a threshold, beyond which late events are considered too late and discarded.\n",
    "\n",
    "In general, the system will track the processing time $P$ as the maximum event seen, and if the difference between $P$ and the event time of the incoming event is smaller, it will be considered on the aggregation and the table will update accordingly, as it had arrived on time.\n",
    "\n",
    "#### Example Scenario: Real-Time Traffic Monitoring\n",
    "\n",
    "Consider a traffic management system monitoring vehicle speeds:\n",
    "\n",
    "* Events include vehicle ID, speed, and timestamp.\n",
    "* Speed averages are calculated over 10-minute windows.\n",
    "* Some sensor data may be delayed due to network latency.\n",
    "\n",
    "Using watermarking:\n",
    "\n",
    "* Late arrivals are allowed up to 5 minutes after the window's end.\n",
    "* Delayed data exceeding this threshold is discarded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Is there an exact incremental algorithm for computing average, median and distinct? Why?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Average: Yes, an exact incremental algorithm exists. It tracks a running sum and count to compute the average efficiently.\n",
    "\n",
    "* Median: No exact incremental algorithm exists without storing all data, as the median requires sorted values. Approximations like quantile summaries can estimate it.\n",
    "\n",
    "* Distinct Count: No exact incremental algorithm exists without tracking all unique elements. Approximations like HyperLogLog are used for scalability.\n",
    "\n",
    "Exact algorithms for median and distinct counts aren't feasible in streaming due to memory and real-time constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is a hopping logical window? Give an example at the conceptual level and show that you know both EPL’s and Spark Structured Streaming’s syntax\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A hopping logical window is a fixed-sized window that slides by a fixed-sized increment. With logical, it means that this size is measured by time. On a window, multiple events can be grouped and processed.\n",
    "\n",
    "EPL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "select sensor\n",
    "from TemperatureSensorEvent.win:time(1 minutes)\n",
    "group by sensor;\n",
    "output snapshot every 30 seconds;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "temperature_df.groupBy(window(\"TS\", \"1 minutes\", \"30 seconds\"), \"SENSOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What is the role of the output clause in EPL? Give an example that supports your explanation.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "It defines how the output stream is going to be generated. It is what we use to, for example, avoid the overwhelming of event generation after certain trigger (event pattern) happens.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "select sensor\n",
    "from FireEvent.win:time(10 minutes);\n",
    "output first every 5 minutes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What’s the role of the followed by operator (->) in EPL? Is it possible to implement it in Spark Structured Streaming? Give an example that supports your explanation.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The specific role of the followed by (->) operator in EPL is to detect specific sequences of events that happen (for example, during a window or context). Combined with the every clause, it allows us to detect and report the ocurrence of one event followed by another.\n",
    "\n",
    "This operator is one of the tools of CEP that allows us to tame the torrent effect by filtering and processing high-level patterns instead of low-level events.\n",
    "\n",
    "In Spark, this operator can only be implemented partially (on the form every A -> every B) by using a stream-to-stream join with temporal constraints on the events timestamps\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "join_sdf = smoke_events . join (\n",
    "    high_temp_events , expr ( \"\"\"\n",
    "        ( sensorTemp == sensorSmoke ) AND\n",
    "        ( tsTemp > tsSmoke ) AND                   ## this is the EPL ’s -> operator\n",
    "        ( tsTemp < tsSmoke + interval 2 minute )   ## this is the timer : within clause\n",
    "    \"\"\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Explain stream to stream joins. Give an example in EPL that illustrates the different results you obtain using an inner and a left outer join\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Stream-to-stream joins allow combining two event streams based on a common key or condition. A window (logical or physical) is used to limit the join to events within a specified time frame.\n",
    "\n",
    "Inner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM View#time(9 sec) AS v \n",
    "INNER JOIN Click#time(9 sec) AS c \n",
    "ON v.id = c.id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Outer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM View#time(9 sec) AS v \n",
    "LEFT OUTER JOIN Click#time(9 sec) AS c \n",
    "ON v.id = c.id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every time that an event arrives, it will trigger the query. On the inner join, an output will be produced if we find a match in both streams within the window. On the left outer, every event on View will be on the output, with a null event in case that no event matched on the Clicks stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Explain stream to table joins. Given an example in EPL that illustrates the different results you obtain using an inner and a left outer join\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Stream-to-table joins allows us to combine the events from a stream with the events grouped on a table, based on a common key. This table is updated dynamically from a stream, by using a grouping clause (for example $\\texttt{Click#unique(id)}$).\n",
    "\n",
    "Note that these types of joins incorporate a keyword $\\texttt{UNIDIRECTIONAL}$ to execute the query only on the arrivals of the stream events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "select *\n",
    "from View as v\n",
    "unidirectional inner join\n",
    "Click # unique ( id ) as c\n",
    "on v . id = c . id ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an left outer join, we incorporate all the results from the stream View."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) What makes Kafka scale horizontally? Explain it using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "On a Kafka cluster, adding a new machine improves the system's efficiency by increasing its capacity and fault tolerance. Kafka divides a cluster into brokers, each hosted on a different machine. Each broker manages a subset of partitions from various topics. Adding brokers allows:\n",
    "\n",
    "* Parallel Reads and Writes: Enables concurrent operations across multiple partitions.\n",
    "* Balanced Distribution: Kafka redistributes partitions across brokers, balancing the load.\n",
    "* Enhanced Fault Tolerance: Replication of partitions ensures data availability, even if one broker fails.\n",
    "* Consumer Scaling: Consumers in a group process partitions in parallel, benefiting from the distributed architecture.\n",
    "\n",
    "This ensures that the Kafka cluster scales effectively with minimal bottlenecks as data volume grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What’s the role of a topic in Kafka? How does it relate to partitions and brokers? Explain it using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A topic is a category used to organize and store messages. Producers send messages to topics, and consumers read from topics, allowing a decoupled communication between these entities. \n",
    "\n",
    "Each topic is divided in subsets called partitions, so many partitions can refer to the same topic. These partitions provide an unit of parallelism and fault-tolerance. A broker can contain one or more partitions from different topics, and they handle the read, write and replication of the data in the partitions.\n",
    "\n",
    "For example, let us suppose we have multiple weather stations, sending the temperature ans humidity of sectors in a city. We can group these in the topic `WeatherReport`. This topic is divided into 2 partitions (e.g., from the zones A to D, and from the zones E to H), and each partition is sent into a broker. We also have 2 other brokers for fault tolerance, where the partitions are replicated. Then, the consumer just needs to subscribe to the topic, and read from the partitions (allowing parallel search, for example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) What’s the role of a broker in Kafka? How does it relate to topics and partitions? Explain it using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A broker in Kafka is responsible of storing one or more partitions from different topics on a Kafka cluster. It handles the reads and writes, as well as distributing the data to other brokers for replication. A broker can have many partitions, from many different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) What’s the role of a consumer group in Kafka? How does it relate to topics and partitions? Explain it using an example.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A consumer group allows distributed consumption of the topics. In other words, it allows the parallel read of messages from a topic, by grouping consumers subscribed to de same topic. \n",
    "\n",
    "A consumer group may have one consumer that receives messages from all partitions of the topic it is subscribed to (bottleneck), or it can have many parallel consumers, each receiving messages from one (or more, but not all) of the partitions of the subscribed topic, with the goal of parallelizing the read.\n",
    "\n",
    "A consumer group can only consume from one topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you receive the following stream of events:\n",
    "\n",
    "$$\\texttt{A1@0,C1@1,B1@2,B2@3,A2@4,B3@5,A3@6,B4@10}$$\n",
    "\n",
    "Note that $\\texttt{A3@6}$ denotes an event of type A identified by the number 3 that\n",
    "is received at time 6.\n",
    "\n",
    "Given the patter:\n",
    "\n",
    "$$\\texttt{every A -> (B and not C where timer:within(3 sec))}$$\n",
    "\n",
    "1) Translate the pattern into an English sentence\n",
    "2) Which are the events that trigger the matching? Why?\n",
    "3) Which are the events that may trigger the matching but are excluded by the semantics of the every and the where timer:within clauses? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "1) For every occurence of an event A, look for an ocurrance of an event B, provided that there is no occurrance of an event C within 3 seconds of A. \n",
    "\n",
    "2) (A2, B3), (A3, B4)\n",
    "\n",
    "3) As we are not using an `every` after the `->` the event (A2, B4) is not captured. Also, the `where timer:within` clause excludes the (A1, B1). The others (A1, B2), (A1, B3) and (A1, B4) are excluded by both semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the git repo of the course, you find a complete example about a\n",
    "Robotic Arm solved in:\n",
    "\n",
    "* EPL:\n",
    "  * https://github.com/Streaming-Data-Analytics/Courseware/tree/main/Streaming%20Data%20Engineering/EPL/epl_robotic-arm\n",
    "\n",
    "* Spark Structured Streaming:\n",
    "  * https://github.com/Streaming-Data-Analytics/Courseware/tree/main/Streaming%20Data%20Engineering/Spark/sss_robotic-arm\n",
    "\n",
    "There are other two examples completely solved in EPL:\n",
    "* https://github.com/Streaming-Data-Analytics/Courseware/tree/main/Streaming%20Data%20Engineering/EPL/epl_tomatopick\n",
    "\n",
    "* https://github.com/Streaming-Data-Analytics/Courseware/tree/main/Streaming%20Data%20Engineering/EPL/epl_bocce (VERY HARD)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
